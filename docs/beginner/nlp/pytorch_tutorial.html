

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <link rel="canonical" href="http://tutorials.pytorch.kr/beginner/nlp/pytorch_tutorial.html" />
  <meta charset="utf-8">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>PyTorch 소개 &mdash; PyTorch Tutorials 0.4.0 documentation</title>

















    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />



    <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />

    <link rel="stylesheet" href="../../_static/css/pytorch_theme.css" type="text/css" />

    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato" type="text/css" />



        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="PyTorch Tutorials 0.4.0 documentation" href="../../index.html"/>
        <link rel="up" title="Deep Learning for NLP with Pytorch" href="../deep_learning_nlp_tutorial.html"/>
        <link rel="next" title="PyTorch를 이용한 딥러닝" href="deep_learning_tutorial.html"/>
        <link rel="prev" title="Deep Learning for NLP with Pytorch" href="../deep_learning_nlp_tutorial.html"/>


  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">


  <div class="wy-grid-for-nav">


    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">



            <a href="../../index.html" class="icon icon-home"> PyTorch Tutorials




            <img src="../../_static/pytorch-logo-dark.svg" class="logo" />

          </a>




              <div class="version">
                0.4.0
              </div>




<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>


        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">






              <p class="caption"><span class="caption-text">Beginner Tutorials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../deep_learning_60min_blitz.html">PyTorch로 딥러닝하기: 60분만에 끝장내기</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../blitz/tensor_tutorial.html">PyTorch가 무엇인가요?</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../blitz/tensor_tutorial.html#id1">시작하기</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../blitz/tensor_tutorial.html#tensors">Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../blitz/tensor_tutorial.html#operations">연산(Operations)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/tensor_tutorial.html#numpy-bridge">NumPy 변환(Bridge)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../blitz/tensor_tutorial.html#torch-tensor-numpy">Torch Tensor를 NumPy 배열로 변환하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="../blitz/tensor_tutorial.html#numpy-torch-tensor">NumPy 배열을 Torch Tensor로 변환하기</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/tensor_tutorial.html#cuda-tensors">CUDA Tensors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../blitz/autograd_tutorial.html">Autograd: 자동 미분</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../blitz/autograd_tutorial.html#tensor">Tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/autograd_tutorial.html#gradient">변화도(Gradient)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../blitz/neural_networks_tutorial.html">신경망(Neural Networks)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../blitz/neural_networks_tutorial.html#id1">신경망 정의하기</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/neural_networks_tutorial.html#loss-function">손실 함수 (Loss Function)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/neural_networks_tutorial.html#backprop">역전파(Backprop)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/neural_networks_tutorial.html#id4">가중치 갱신</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../blitz/cifar10_tutorial.html">분류기(Classifier) 학습하기</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../blitz/cifar10_tutorial.html#id1">데이터는 어떻게 하나요?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/cifar10_tutorial.html#id2">이미지 분류기 학습하기</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../blitz/cifar10_tutorial.html#cifar10">1. CIFAR10를 불러오고 정규화하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="../blitz/cifar10_tutorial.html#convolution-neural-network">2. 합성곱 신경망(Convolution Neural Network) 정의하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="../blitz/cifar10_tutorial.html#optimizer">3. 손실 함수와 Optimizer 정의하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="../blitz/cifar10_tutorial.html#id3">4. 신경망 학습하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="../blitz/cifar10_tutorial.html#id4">5. 시험용 데이터로 신경망 검사하기</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/cifar10_tutorial.html#gpu">GPU에서 학습하기</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/cifar10_tutorial.html#id5">여러개의 GPU에서 학습하기</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/cifar10_tutorial.html#id6">이제 뭘 해볼까요?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../blitz/data_parallel_tutorial.html">Optional: Data Parallelism</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../blitz/data_parallel_tutorial.html#imports-and-parameters">Imports and parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/data_parallel_tutorial.html#dummy-dataset">Dummy DataSet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/data_parallel_tutorial.html#simple-model">Simple Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/data_parallel_tutorial.html#create-model-and-dataparallel">Create Model and DataParallel</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/data_parallel_tutorial.html#run-the-model">Run the Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/data_parallel_tutorial.html#results">Results</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../blitz/data_parallel_tutorial.html#gpus">2 GPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../blitz/data_parallel_tutorial.html#id1">3 GPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../blitz/data_parallel_tutorial.html#id2">8 GPUs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../blitz/data_parallel_tutorial.html#summary">Summary</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../former_torchies_tutorial.html">Torch 사용자를 위한 PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../former_torchies/tensor_tutorial.html">Tensor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/tensor_tutorial.html#in-place-out-of-place">In-place / Out-of-place</a></li>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/tensor_tutorial.html#zero-indexing">0-인덱스(Zero Indexing)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/tensor_tutorial.html#camel-case">카멜표기법(Camel Case) 없음</a></li>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/tensor_tutorial.html#numpy-bridge">NumPy 변환(Bridge)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../former_torchies/tensor_tutorial.html#torch-tensor-numpy">Torch Tensor를 NumPy 배열로 변환하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="../former_torchies/tensor_tutorial.html#numpy-torch-tensor">NumPy 배열을 Torch Tensor로 변환하기</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/tensor_tutorial.html#cuda-tensors">CUDA Tensors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../former_torchies/autograd_tutorial.html">Autograd</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/autograd_tutorial.html#history-track-tensor">이력(history)을 추적(track)하는 Tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/autograd_tutorial.html#gradient">변화도(Gradient)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../former_torchies/nn_tutorial.html">nn 패키지</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/nn_tutorial.html#convnet">예제1: 합성곱 신경망(ConvNet)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/nn_tutorial.html#hook">순방향/역방향 함수 훅(Hook)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/nn_tutorial.html#recurrent-nets">예제2: 순환 신경망(Recurrent Nets)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../former_torchies/parallelism_tutorial.html">멀티-GPU 예제</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/parallelism_tutorial.html#dataparallel">DataParallel</a></li>
<li class="toctree-l3"><a class="reference internal" href="../former_torchies/parallelism_tutorial.html#cpu-gpu">모델의 일부는 CPU, 일부는 GPU에서</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch_with_examples.html">예제로 배우는 PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch_with_examples.html#tensor">Tensor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#numpy">준비 운동: NumPy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#pytorch-tensor">PyTorch: Tensor</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch_with_examples.html#autograd">Autograd</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#pytorch-tensor-autograd">PyTorch: Tensor와 autograd</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#pytorch-autograd">PyTorch: 새 autograd 함수 정의하기</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#tensorflow-static-graph">TensorFlow: 정적 그래프(Static Graph)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch_with_examples.html#nn"><cite>nn</cite> 모듈</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#pytorch-nn">PyTorch: nn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#pytorch-optim">PyTorch: optim</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#id3">PyTorch: 사용자 정의 nn 모듈</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#pytorch-control-flow-weight-sharing">PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch_with_examples.html#examples-download">예제 코드</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#id5">Tensor</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples_tensor/two_layer_net_numpy.html">준비 운동: NumPy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples_tensor/two_layer_net_tensor.html">PyTorch: Tensor</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#id6">Autograd</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples_autograd/two_layer_net_autograd.html">PyTorch: Tensor와 autograd</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples_autograd/two_layer_net_custom_function.html">PyTorch: 새 autograd 함수 정의하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples_autograd/tf_two_layer_net.html">TensorFlow: 정적 그래프(Static Graph)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch_with_examples.html#id7"><cite>nn</cite> 모듈</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples_nn/two_layer_net_nn.html">PyTorch: nn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples_nn/two_layer_net_optim.html">PyTorch: optim</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples_nn/two_layer_net_module.html">PyTorch: 사용자 정의 nn 모듈</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples_nn/dynamic_net.html">PyTorch: 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../transfer_learning_tutorial.html">전이학습(Transfer Learning) 튜토리얼</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../transfer_learning_tutorial.html#id2">데이터 불러오기</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../transfer_learning_tutorial.html#id4">일부 이미지 시각화하기</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../transfer_learning_tutorial.html#id5">모델 학습하기</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../transfer_learning_tutorial.html#id6">모델 예측값 시각화하기</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../transfer_learning_tutorial.html#finetuning">합성곱 신경망 미세조정(Finetuning)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../transfer_learning_tutorial.html#id7">학습 및 평가하기</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../transfer_learning_tutorial.html#id8">고정 특정 추출기로써의 합성곱 신경망</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../transfer_learning_tutorial.html#id9">학습 및 평가하기</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../data_loading_tutorial.html">Data Loading and Processing Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../data_loading_tutorial.html#dataset-class">Dataset class</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_loading_tutorial.html#transforms">Transforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../data_loading_tutorial.html#compose-transforms">Compose transforms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../data_loading_tutorial.html#iterating-through-the-dataset">Iterating through the dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../data_loading_tutorial.html#afterword-torchvision">Afterword: torchvision</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../deep_learning_nlp_tutorial.html">Deep Learning for NLP with Pytorch</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">PyTorch 소개</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#torch-tensor">Torch의 텐서(Tensor) 라이브러리 소개</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">텐서 생성하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">텐서로 작업하기</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">텐서 재구성</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#computation-graph-automatic-differentiation">연산 그래프(Computation Graph)와 자동 미분(Automatic Differentiation)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="deep_learning_tutorial.html">PyTorch를 이용한 딥러닝</a><ul>
<li class="toctree-l3"><a class="reference internal" href="deep_learning_tutorial.html#affine-maps">딥러닝 블록 구축 : 아핀 맵(affine maps), 비선형성, 객체</a><ul>
<li class="toctree-l4"><a class="reference internal" href="deep_learning_tutorial.html#id1">아핀 맵</a></li>
<li class="toctree-l4"><a class="reference internal" href="deep_learning_tutorial.html#id2">비선형성</a></li>
<li class="toctree-l4"><a class="reference internal" href="deep_learning_tutorial.html#softmax">Softmax 및 확률</a></li>
<li class="toctree-l4"><a class="reference internal" href="deep_learning_tutorial.html#objective-functions">목적 함수(Objective Functions)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="deep_learning_tutorial.html#id3">최적화와 학습</a></li>
<li class="toctree-l3"><a class="reference internal" href="deep_learning_tutorial.html#id4">Pytorch 에서 네트워크 구성요소 생성하기</a><ul>
<li class="toctree-l4"><a class="reference internal" href="deep_learning_tutorial.html#bag-of-words">예제: 논리 회귀 Bag-of-Words 분류기</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="word_embeddings_tutorial.html">Word Embeddings: Encoding Lexical Semantics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="word_embeddings_tutorial.html#getting-dense-word-embeddings">Getting Dense Word Embeddings</a></li>
<li class="toctree-l3"><a class="reference internal" href="word_embeddings_tutorial.html#word-embeddings-in-pytorch">Word Embeddings in Pytorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="word_embeddings_tutorial.html#an-example-n-gram-language-modeling">An Example: N-Gram Language Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="word_embeddings_tutorial.html#exercise-computing-word-embeddings-continuous-bag-of-words">Exercise: Computing Word Embeddings: Continuous Bag-of-Words</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="sequence_models_tutorial.html">Sequence Models and Long-Short Term Memory Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="sequence_models_tutorial.html#lstm-s-in-pytorch">LSTM’s in Pytorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="sequence_models_tutorial.html#example-an-lstm-for-part-of-speech-tagging">Example: An LSTM for Part-of-Speech Tagging</a></li>
<li class="toctree-l3"><a class="reference internal" href="sequence_models_tutorial.html#exercise-augmenting-the-lstm-part-of-speech-tagger-with-character-level-features">Exercise: Augmenting the LSTM part-of-speech tagger with character-level features</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="advanced_tutorial.html">Advanced: Making Dynamic Decisions and the Bi-LSTM CRF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="advanced_tutorial.html#dynamic-versus-static-deep-learning-toolkits">Dynamic versus Static Deep Learning Toolkits</a></li>
<li class="toctree-l3"><a class="reference internal" href="advanced_tutorial.html#bi-lstm-conditional-random-field-discussion">Bi-LSTM Conditional Random Field Discussion</a></li>
<li class="toctree-l3"><a class="reference internal" href="advanced_tutorial.html#implementation-notes">Implementation Notes</a></li>
<li class="toctree-l3"><a class="reference internal" href="advanced_tutorial.html#exercise-a-new-loss-function-for-discriminative-tagging">Exercise: A new loss function for discriminative tagging</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Intermediate Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html">문자-단위 RNN으로 이름 분류하기</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#id2">데이터 준비하기</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#tensor">이름을 Tensor 로 변경</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#id3">네트워크 생성</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#id4">학습</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#id5">학습 준비</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#id6">네트워크 학습</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#id7">결과 도식화</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#id8">결과 평가</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#id9">사용자 입력으로 실행</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html#id10">연습</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html">문자-단위 RNN으로 이름 생성하기</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html#id2">데이터 준비하기</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html#id4">네트워크 생성</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html#id5">학습</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html#id6">학습 준비</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html#id7">네트워크 학습</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html#id8">손실 도식화</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html#id9">네트워크 샘플링</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html">Sequence to Sequence 네트워크와 Attention을 이용한 번역</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#id2">데이터 파일 로딩</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#seq2seq">Seq2Seq 모델</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#id4">인코더</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#id5">디코더</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#id6">간단한 디코더</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#id7">어텐션 디코더</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#id8">학습</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#id9">학습 데이터 준비</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#id10">모델 학습</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#id11">결과 도식화</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#id12">평가</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#id13">학습과 평가</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#id14">어텐션 시각화</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html#id15">연습</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html">강화 학습 (DQN) 튜토리얼</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html#replay-memory">재현 메모리(Replay Memory)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html#id2">DQN 알고리즘</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html#q">Q-네트워크</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html#id3">입력 추출</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html#id4">학습</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html#id5">하이퍼 파라미터와 유틸리티</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html#id8">학습 루프</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/dist_tuto.html">Pytorch로 분산 어플리케이션 개발하기</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/dist_tuto.html#setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/dist_tuto.html#point-to-point-communication">지점간 통신(Point-to-Point Communication)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/dist_tuto.html#collective-communication">집단 통신 (Collective Communication)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/dist_tuto.html#distributed-training">분산 학습(Distributed Training)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/dist_tuto.html#our-own-ring-allreduce">Our Own Ring-Allreduce</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/dist_tuto.html#advanced-topics">Advanced Topics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/dist_tuto.html#id2">통신 백엔드</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../intermediate/dist_tuto.html#id3">초기화 방법</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/spatial_transformer_tutorial.html">공간 변형기 네트워크(Spatial Transformer Networks) 튜토리얼</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/spatial_transformer_tutorial.html#id2">데이터 로딩</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/spatial_transformer_tutorial.html#id3">공간 변형 네트워크 설명</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/spatial_transformer_tutorial.html#id4">모델 학습</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intermediate/spatial_transformer_tutorial.html#stn">STN 결과 시각화</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Advanced Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/neural_style_tutorial.html">PyTorch를 이용한 신경망-변환(Neural-Transfer)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#id2">소개</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#id3">신경망 뭐라고?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#id4">어떻게 동작합니까?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#id5">그래서. 어떻게 동작하냐고요?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#pytorch">PyTorch 구현</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#id6">패키지들</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#cuda">쿠다(CUDA)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#id7">이미지 읽기</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#id8">이미지 표시하기</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#id9">콘텐츠 로스</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#id10">스타일 로스</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#id15">뉴럴 네트워크 읽기</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#id16">입력 이미지</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/neural_style_tutorial.html#id17">경사 하강법</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/numpy_extensions_tutorial.html">Creating extensions using numpy and scipy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/numpy_extensions_tutorial.html#parameter-less-example">Parameter-less example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/numpy_extensions_tutorial.html#parametrized-example">Parametrized example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/super_resolution_with_caffe2.html">Transfering a model from PyTorch to Caffe2 and Mobile using ONNX</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/super_resolution_with_caffe2.html#transfering-srresnet-using-onnx">Transfering SRResNet using ONNX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/super_resolution_with_caffe2.html#running-the-model-on-mobile-devices">Running the model on mobile devices</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_extension.html">Custom C++ and CUDA Extensions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/cpp_extension.html#motivation-and-example">Motivation and Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/cpp_extension.html#writing-a-c-extension">Writing a C++ Extension</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/cpp_extension.html#building-with-setuptools">Building with <code class="docutils literal notranslate"><span class="pre">setuptools</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/cpp_extension.html#writing-the-c-op">Writing the C++ Op</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../advanced/cpp_extension.html#forward-pass">Forward Pass</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../advanced/cpp_extension.html#backward-pass">Backward Pass</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/cpp_extension.html#binding-to-python">Binding to Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/cpp_extension.html#using-your-extension">Using Your Extension</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../advanced/cpp_extension.html#performance-comparison">Performance Comparison</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../advanced/cpp_extension.html#performance-on-gpu-devices">Performance on GPU Devices</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/cpp_extension.html#jit-compiling-extensions">JIT Compiling Extensions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/cpp_extension.html#writing-a-mixed-c-cuda-extension">Writing a Mixed C++/CUDA extension</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../advanced/cpp_extension.html#integrating-a-c-cuda-operation-with-pytorch">Integrating a C++/CUDA Operation with PyTorch</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../advanced/cpp_extension.html#id2">Performance Comparison</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../advanced/cpp_extension.html#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>



        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">


      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">

          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">PyTorch Tutorials</a>

      </nav>



      <div class="wy-nav-content">
        <div class="rst-content">
















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">

      <li><a href="../../index.html">Docs</a> &raquo;</li>

          <li><a href="../deep_learning_nlp_tutorial.html">Deep Learning for NLP with Pytorch</a> &raquo;</li>

      <li>PyTorch 소개</li>


      <li class="wy-breadcrumbs-aside">


            <a href="../../_sources/beginner/nlp/pytorch_tutorial.rst.txt" rel="nofollow"> View page source</a>


      </li>

  </ul>


  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">

  <div class="section" id="pytorch">
<span id="sphx-glr-beginner-nlp-pytorch-tutorial-py"></span><h1>PyTorch 소개<a class="headerlink" href="#pytorch" title="Permalink to this headline">¶</a></h1>
<div class="section" id="torch-tensor">
<h2>Torch의 텐서(Tensor) 라이브러리 소개<a class="headerlink" href="#torch-tensor" title="Permalink to this headline">¶</a></h2>
<p>모든 딥러닝은 2차원 이상으로 색인될 수 있는 행렬의 일반화인
텐서의 연산입니다. 이것이 무엇을 의미하지 나중에 정확히
알게 될 것입니다. 먼저, 텐서로 무엇을 할 수 있는지 살펴 봅시다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Author: Robert Guthrie</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.autograd</span> <span class="kn">as</span> <span class="nn">autograd</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="kn">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="kn">as</span> <span class="nn">optim</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="id1">
<h3>텐서 생성하기<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>텐서는 파이썬 리스트에서 torch.Tensor() 함수로 생성될 수 있습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># torch.tensor(data) 는 주어진 데이터로 torch.Tensor 객체를 생성합니다.</span>
<span class="n">V_data</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">V_data</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>

<span class="c1"># 행렬 생성</span>
<span class="n">M_data</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">M_data</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>

<span class="c1"># 2x2x2 크기의 3D 텐서 생성.</span>
<span class="n">T_data</span> <span class="o">=</span> <span class="p">[[[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]],</span>
          <span class="p">[[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">],</span> <span class="p">[</span><span class="mf">7.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">]]]</span>
<span class="n">T</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">T_data</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">])</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">4.</span><span class="p">,</span>  <span class="mf">5.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">]])</span>
<span class="n">tensor</span><span class="p">([[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">3.</span><span class="p">,</span>  <span class="mf">4.</span><span class="p">]],</span>

        <span class="p">[[</span> <span class="mf">5.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">7.</span><span class="p">,</span>  <span class="mf">8.</span><span class="p">]]])</span>
</pre></div>
</div>
<p>어쨌든 3D 텐서가 무엇입니까? 이렇게 생각해 보십시오. 만약 벡터가 있다면
벡터에 주소를 입력하면 스칼라를 줍니다. 만약 행렬을 가지고 있다면 행렬에
주소를 입력하면 벡터를 줍니다. 만약 3D 텐서를 가지고 있다면 텐서에 주소를
입력하면 행렬을 줍니다.</p>
<p>용어에 대한 주석:
이 튜토리얼에서 “텐서”를 언급 할 때 그것은 어떤 torch.Tensor 객체를 말합니다.
행렬과 벡터는 각각 차원이 1과 2인 torch.Tensors 의 특별한 케이스 입니다.
3D 텐서를 말할 때는 “3D 텐서”라고 명시적으로 사용하겠습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Index into V and get a scalar (0 dimensional tensor)</span>
<span class="k">print</span><span class="p">(</span><span class="n">V</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># Get a Python number from it</span>
<span class="k">print</span><span class="p">(</span><span class="n">V</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="c1"># Index into M and get a vector</span>
<span class="k">print</span><span class="p">(</span><span class="n">M</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Index into T and get a matrix</span>
<span class="k">print</span><span class="p">(</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">)</span>
<span class="mf">1.0</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">])</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">3.</span><span class="p">,</span>  <span class="mf">4.</span><span class="p">]])</span>
</pre></div>
</div>
<p>다른 데이터 유형의 텐서를 생성 할 수도 있습니다. 보시다시피 기본값은
Float입니다. 정수형의 텐서를 만들려면 torch.LongTensor ()를 사용하십시오.
더 많은 데이터 유형에 대해서는 설명서를 확인하십시오.
그러나 Float 및 Long이 가장 일반적입니다.</p>
<p>torch.randn ()을 사용하여 랜덤 데이터와 제공된 차원으로 텐서를
만들 수 있습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([[[</span><span class="o">-</span><span class="mf">1.5256</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7502</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6540</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6095</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1002</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.6092</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9798</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6091</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7121</span><span class="p">,</span>  <span class="mf">0.3037</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.7773</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2515</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2223</span><span class="p">,</span>  <span class="mf">1.6871</span><span class="p">,</span>  <span class="mf">0.2284</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.4676</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6970</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1608</span><span class="p">,</span>  <span class="mf">0.6995</span><span class="p">,</span>  <span class="mf">0.1991</span><span class="p">]],</span>

        <span class="p">[[</span> <span class="mf">0.8657</span><span class="p">,</span>  <span class="mf">0.2444</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6629</span><span class="p">,</span>  <span class="mf">0.8073</span><span class="p">,</span>  <span class="mf">1.1017</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.1759</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.2456</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4465</span><span class="p">,</span>  <span class="mf">0.0612</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6177</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.7981</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1316</span><span class="p">,</span>  <span class="mf">1.8793</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0721</span><span class="p">,</span>  <span class="mf">0.1578</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.7735</span><span class="p">,</span>  <span class="mf">0.1991</span><span class="p">,</span>  <span class="mf">0.0457</span><span class="p">,</span>  <span class="mf">0.1530</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4757</span><span class="p">]],</span>

        <span class="p">[[</span><span class="o">-</span><span class="mf">0.1110</span><span class="p">,</span>  <span class="mf">0.2927</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1578</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0288</span><span class="p">,</span>  <span class="mf">0.4533</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.1422</span><span class="p">,</span>  <span class="mf">0.2486</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7754</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0255</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0233</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.5962</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0055</span><span class="p">,</span>  <span class="mf">0.4285</span><span class="p">,</span>  <span class="mf">1.4761</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7869</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">1.6103</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7040</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1853</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9962</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8313</span><span class="p">]]])</span>
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h3>텐서로 작업하기<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>기대하는 방식으로 텐서로 작업할 수 있습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">])</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([</span> <span class="mf">5.</span><span class="p">,</span>  <span class="mf">7.</span><span class="p">,</span>  <span class="mf">9.</span><span class="p">])</span>
</pre></div>
</div>
<p>사용 가능한 방대한 작업의 전체 목록은
<a class="reference external" href="http://pytorch.org/docs/torch.html">문서</a> 를 참고하십시오. 단순한
수학적 연산 이상으로 확장됩니다.</p>
<p>나중에 사용하게 될 유용한 작업 중 하나는 연결입니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 기본으로 첫번째 축(가로 연결)을 따라 연결합니다.</span>
<span class="n">x_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">y_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">z_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x_1</span><span class="p">,</span> <span class="n">y_1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">z_1</span><span class="p">)</span>

<span class="c1"># 세로 연결:</span>
<span class="n">x_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">y_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="c1"># 두번째 변수는 연결될 축을 결정합니다.</span>
<span class="n">z_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x_2</span><span class="p">,</span> <span class="n">y_2</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">z_2</span><span class="p">)</span>

<span class="c1"># 텐서가 호환되지 않으면 Torch가 오류 메시지를 출력 합니다. 주석 처리를 제거하여 오류를 확인하십시오.</span>
<span class="c1"># torch.cat([x_1, x_2])</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8029</span><span class="p">,</span>  <span class="mf">0.2366</span><span class="p">,</span>  <span class="mf">0.2857</span><span class="p">,</span>  <span class="mf">0.6898</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6331</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.8795</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6842</span><span class="p">,</span>  <span class="mf">0.4533</span><span class="p">,</span>  <span class="mf">0.2912</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8317</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.5525</span><span class="p">,</span>  <span class="mf">0.6355</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3968</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6571</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6428</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.9803</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0421</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8206</span><span class="p">,</span>  <span class="mf">0.3133</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1352</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.3773</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2824</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5667</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4303</span><span class="p">,</span>  <span class="mf">0.5009</span><span class="p">]])</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.5438</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4057</span><span class="p">,</span>  <span class="mf">1.1341</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1473</span><span class="p">,</span>  <span class="mf">0.6272</span><span class="p">,</span>  <span class="mf">1.0935</span><span class="p">,</span>  <span class="mf">0.0939</span><span class="p">,</span>
          <span class="mf">1.2381</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.1115</span><span class="p">,</span>  <span class="mf">0.3501</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.7703</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.3459</span><span class="p">,</span>  <span class="mf">0.5119</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6933</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1668</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">0.9999</span><span class="p">]])</span>
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h3>텐서 재구성<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>.view() 메서드를 사용해서 텐서를 재구성합니다.
이 메서드는 많은 신경망 구성 요소가 입력을 특정 모양으로 예상하기
때문에 많이 사용됩니다. 데이터를 구성 요소로 전달하기 전에 종종 모양을
변경해야합니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">12</span><span class="p">))</span>  <span class="c1"># 가로 2 , 세로 12로 재구성</span>
<span class="c1"># 위와 같이 하나의 차원이 -1이면 그 것의 크기는 유추될 수 있습니다.</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([[[</span> <span class="mf">0.4175</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2127</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8400</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4200</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.6240</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9773</span><span class="p">,</span>  <span class="mf">0.8748</span><span class="p">,</span>  <span class="mf">0.9873</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">0.0594</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.4919</span><span class="p">,</span>  <span class="mf">0.2423</span><span class="p">,</span>  <span class="mf">0.2883</span><span class="p">]],</span>

        <span class="p">[[</span><span class="o">-</span><span class="mf">0.1095</span><span class="p">,</span>  <span class="mf">0.3126</span><span class="p">,</span>  <span class="mf">1.5038</span><span class="p">,</span>  <span class="mf">0.5038</span><span class="p">],</span>
         <span class="p">[</span> <span class="mf">0.6223</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4481</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2856</span><span class="p">,</span>  <span class="mf">0.3880</span><span class="p">],</span>
         <span class="p">[</span><span class="o">-</span><span class="mf">1.1435</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6512</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1032</span><span class="p">,</span>  <span class="mf">0.6937</span><span class="p">]]])</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.4175</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2127</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8400</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4200</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6240</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9773</span><span class="p">,</span>  <span class="mf">0.8748</span><span class="p">,</span>
          <span class="mf">0.9873</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0594</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.4919</span><span class="p">,</span>  <span class="mf">0.2423</span><span class="p">,</span>  <span class="mf">0.2883</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1095</span><span class="p">,</span>  <span class="mf">0.3126</span><span class="p">,</span>  <span class="mf">1.5038</span><span class="p">,</span>  <span class="mf">0.5038</span><span class="p">,</span>  <span class="mf">0.6223</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4481</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2856</span><span class="p">,</span>
          <span class="mf">0.3880</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1435</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6512</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1032</span><span class="p">,</span>  <span class="mf">0.6937</span><span class="p">]])</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.4175</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2127</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8400</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4200</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6240</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.9773</span><span class="p">,</span>  <span class="mf">0.8748</span><span class="p">,</span>
          <span class="mf">0.9873</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0594</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.4919</span><span class="p">,</span>  <span class="mf">0.2423</span><span class="p">,</span>  <span class="mf">0.2883</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.1095</span><span class="p">,</span>  <span class="mf">0.3126</span><span class="p">,</span>  <span class="mf">1.5038</span><span class="p">,</span>  <span class="mf">0.5038</span><span class="p">,</span>  <span class="mf">0.6223</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.4481</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2856</span><span class="p">,</span>
          <span class="mf">0.3880</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1435</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.6512</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1032</span><span class="p">,</span>  <span class="mf">0.6937</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="computation-graph-automatic-differentiation">
<h2>연산 그래프(Computation Graph)와 자동 미분(Automatic Differentiation)<a class="headerlink" href="#computation-graph-automatic-differentiation" title="Permalink to this headline">¶</a></h2>
<p>연산 그래프의 개념은 역전파 그라디언트를 직접 작성할 필요가 없게 해주기 때문에
효율적인 딥러닝 프로그래밍에 필수적입니다.
연산 그래프는 데이터를 결합하여 출력을 제공하는 방법의 간단한 특징입니다.
그래프는 어떤 연산과 어떤 매개 변수가 연관되는지를 완전하게 특정하기 때문에
도함수(derivative)를 계산하기에 충분한 정보를 포함합니다.
아마 모호하게 들릴지도 모르니, 근본적인 플래그``requires_grad`` 를 사용하는데
어떤 일이 일어나는지 살펴봅시다.</p>
<p>먼저 프로그래머 관점에서 생각해 보세요.
torch 에 무엇이 저장되있나요.
위에서 생성한 텐서의 객체는 무엇입니까? 분명히 데이터와 형태, 그리고
아마 몇몇 다른 것들 입니다. 그러나 우리가 두개의 텐서를 더할 때
출력 텐서를 얻습니다. 이 모든 출력 텐서는 그것의 데이터와 형태를 알고 있습니다.
하지만 그것이 두 텐서의 합이 었는지는 알지 못합니다.(파일에서 읽었을 수도
있고 다른 연산의 결과일 수도 있음)</p>
<p>만일 <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> 라면, 텐서 객체는 것이 어떻게 생성되었는지 추적합니다.
실제로 그것을 확인해 봅시다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 텐서 공장 메소드에 ``requires_grad`` 플래그가 있습니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># requires_grad=True 으로 이전에 있었던 모든 작업을 여전히 할 수 있습니다.</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="c1"># 그러나 z 는 몇가지를 추가로 알고 있습니다.</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([</span> <span class="mf">5.</span><span class="p">,</span>  <span class="mf">7.</span><span class="p">,</span>  <span class="mf">9.</span><span class="p">])</span>
<span class="o">&lt;</span><span class="n">AddBackward1</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7f6512f06c50</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>그래서 텐서는 무엇이 그들을 생성했는지를 알고 있습니다.
z 는 그것이 파일에서 읽어온 것이 아니고 곱셈 지수승 또는 다른 어떤 것의
결과가 아니라는 것을 알고 있습니다. 그리고 만약 z.grad_fn 를 따라 간다면
x 와 y 를 찾을 것 입니다.</p>
<p>그러나 그것이 기울기(gradient)를 계산하는데 어떻게 도움이 될까요?</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># z 의 모든 요소를 합해 봅시다.</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">s</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">(</span><span class="mf">21.</span><span class="p">)</span>
<span class="o">&lt;</span><span class="n">SumBackward0</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7f649fce6400</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>이제 x의 첫번째 구성요소에 해당하는 이 합계의 도함수가 무엇입니까?
우리가 원하는 수식은 다음과 같습니다.</p>
<div class="math notranslate">
\[\frac{\partial s}{\partial x_0}\]</div>
<p>그럼 s 는 텐서 z 의 합으로 생성되었다는 것을 알고 있습니다.
z 는 x + y 합이 었다는 것을 알고 있습니다. 따라서</p>
<div class="math notranslate">
\[s = \overbrace{x_0 + y_0}^\text{$z_0$} + \overbrace{x_1 + y_1}^\text{$z_1$} + \overbrace{x_2 + y_2}^\text{$z_2$}\]</div>
<p>그리고 따라서 s 는 우리가 원하는 도함수가 1인 것을 결정하는데 충분한 정보를 가지고 있습니다.</p>
<p>물론 이것은 그 도함수를 실제로 계산법에 대한 도전에 대해 해설하고 있습니다.
여기서 핵심은 s 가 그것을 계산 할 수 있는 충분한 정보를 가지고 있다는 것 입니다.
실제로 Pytorch 개발자는 기울기를 계산하는 방법을 알고 sum() 과 + 연산을 프로그램하고
역전파 알고리즘을 실행합니다. 이 알고리즘에 대한 자세한 설명은 이 튜토리얼의
범위를 벗어납니다.</p>
<p>Pytorch가 기울기를 계산하게 하고, 우리가 옳았다는 것을 확인하십시오
( 만약 이 블럭을 여러번 실행한다면 기울기가 증가할 것입니다.
그것은 Pytorch 가 .grad 속성에 기울기를 누적하기 때문이고,
이것은 많은 모델에서 매우 편리합니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 어떤 변수에서 .backward() 호출은 그것에서 시작하는 역전파를 실행합니다.</span>
<span class="n">s</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">,</span>  <span class="mf">1.</span><span class="p">])</span>
</pre></div>
</div>
<p>아래 블록에서 진행되는 작업을 이해하는 것은 딥러닝의 성공적인
프로그래머가 되는데 중요합니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># 기본적으로 사용자가 생성한 텐서는 ``requires_grad=False`` 로 설정됩니다.</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="c1"># 따라서 z 를 통하는 역전파를 계산할 수 없습니다.</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>

<span class="c1"># ``.requires_grad_( ... )`` 는 기존 텐서의 ``requires_grad`` 플래그를 변경합니다.</span>
<span class="c1"># 입력 플래그는 주어지지 않을 경우 기본적으로 ``True`` 설정됩니다.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="c1"># 위에서 본대로 z 는 기울기를 계산하는데 충분한 정보를 가지고 있습니다.</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
<span class="c1"># 만일 연산의 어떤 입력이 ``requires_grad=True`` 를 가지면, 그 출력도 동일합니다.</span>
<span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="c1"># 이제 z는 x와 y와 관련이 있는 연산 이력을 가집니다.</span>
<span class="c1"># 그것의 값만 취해서 그것의 이력에서 **분리** 할 수 있을 까요?</span>
<span class="n">new_z</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

<span class="c1"># ... new_z는 x와 y에 역전파하기 위한 정보를 가지고 있습니까?</span>
<span class="c1"># 아닙니다!</span>
<span class="k">print</span><span class="p">(</span><span class="n">new_z</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
<span class="c1"># 어떻게 그럴 수 있을까요?  ``z.detach()`` 는 ``z`` 와 같은 저장 공간을 공유하지만</span>
<span class="c1"># 계산 이력을 잊어버린 텐서를 반환합니다. 그것은 그것이 어떻게 계산되었는지</span>
<span class="c1"># 아무 것도 모릅니다.</span>
<span class="c1"># 기본적으로 우리는 과거의 이력에서 Tensor를 분리했습니다.</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kc">False</span> <span class="kc">False</span>
<span class="kc">None</span>
<span class="o">&lt;</span><span class="n">AddBackward1</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7f649ff6c4e0</span><span class="o">&gt;</span>
<span class="kc">True</span>
<span class="kc">None</span>
</pre></div>
</div>
<p>requires_grad=True 를 가진 텐서를 <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">torch.no_grad():</span></code> 으로
둘러싸서 이력 추적의 자동미분(autograd)을 멈출 수 있습니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="k">print</span><span class="p">((</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">print</span><span class="p">((</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kc">True</span>
<span class="kc">True</span>
<span class="kc">False</span>
</pre></div>
</div>
<p><strong>Total running time of the script:</strong> ( 0 minutes  0.007 seconds)</p>
<div class="sphx-glr-footer docutils container">
<div class="sphx-glr-download docutils container">
<a class="reference download internal" href="../../_downloads/pytorch_tutorial.py" download=""><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">pytorch_tutorial.py</span></code></a></div>
<div class="sphx-glr-download docutils container">
<a class="reference download internal" href="../../_downloads/pytorch_tutorial.ipynb" download=""><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">pytorch_tutorial.ipynb</span></code></a></div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.readthedocs.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


           </div>
           <div class="articleComments">

           </div>
          </div>
          <footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">

        <a href="deep_learning_tutorial.html" class="btn btn-neutral float-right" title="PyTorch를 이용한 딥러닝" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>


        <a href="../deep_learning_nlp_tutorial.html" class="btn btn-neutral" title="Deep Learning for NLP with Pytorch" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>

    </div>


  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, PyTorch.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.

</footer>

        </div>
      </div>

    </section>

  </div>





    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'0.4.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>





    <script type="text/javascript" src="../../_static/js/theme.js"></script>




  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-71919972-3', 'auto');
  ga('send', 'pageview');

</script>


</body>
</html>