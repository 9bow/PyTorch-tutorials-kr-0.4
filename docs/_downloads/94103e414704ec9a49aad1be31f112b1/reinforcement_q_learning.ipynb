{
  "cells": [
    {
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null
    },
    {
      "source": [
        "\n\uac15\ud654 \ud559\uc2b5 (DQN) \ud29c\ud1a0\ub9ac\uc5bc\n=====================================\n**Author**: `Adam Paszke <https://github.com/apaszke>`_\n  **\ubc88\uc5ed**: `\ud669\uc131\uc218 <https://github.com/adonisues>`_\n\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 `OpenAI Gym <https://gym.openai.com/>`__ \nCartPole-v0 \ud0dc\uc2a4\ud06c\uc758 DQN (Deep Q Learning) \uc5d0\uc774\uc804\ud2b8\ub97c \ud559\uc2b5\ud558\ub294\ub370\nPyTorch\ub97c \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\ub4dc\ub9bd\ub2c8\ub2e4.\n\n**\ud0dc\uc2a4\ud06c**\n\n\uc5d0\uc774\uc804\ud2b8\ub294 \uc5f0\uacb0\ub41c \ub9c9\ub300\uac00 \ub611\ubc14\ub85c \uc11c \uc788\ub3c4\ub85d \uce74\ud2b8\ub97c \uc67c\ucabd\uc774\ub098 \uc624\ub978\ucabd\uc73c\ub85c \n\uc6c0\uc9c1\uc774\ub294 \ub450 \uac00\uc9c0 \ub3d9\uc791 \uc911 \ud558\ub098\ub97c \uc120\ud0dd\ud574\uc57c\ud569\ub2c8\ub2e4. \n\ub2e4\uc591\ud55c \uc54c\uace0\ub9ac\uc998\uacfc \uc2dc\uac01\ud654 \uae30\ub2a5\uc744 \uac16\ucd98 \uacf5\uc2dd \uc21c\uc704\ud45c\ub97c \n`Gym website <https://gym.openai.com/envs/CartPole-v0>`__ \uc5d0\uc11c \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n.. figure:: /_static/img/cartpole.gif\n   :alt: cartpole\n\n   cartpole\n\n\uc5d0\uc774\uc804\ud2b8\uac00 \ud604\uc7ac \ud658\uacbd \uc0c1\ud0dc\ub97c \uad00\ucc30\ud558\uace0 \ud589\ub3d9\uc744 \uc120\ud0dd\ud558\uba74 \n\ud658\uacbd\uc774 \uc0c8\ub85c\uc6b4 \uc0c1\ud0dc\ub85c *\uc804\ud658* \ub418\uace0 \uc791\uc5c5\uc758 \uacb0\uacfc\ub97c \ub098\ud0c0\ub0b4\ub294 \ubcf4\uc0c1\ub3c4 \ubc18\ud658\ub429\ub2c8\ub2e4. \n\uc774 \ud0dc\uc2a4\ud06c\uc5d0\uc11c\ub294 \ub9c9\ub300\uac00 \uc9c0\ub098\uce58\uac8c \ub5a8\uc5b4\uc9c0\uba74 \ud658\uacbd\uc774 \uc885\ub8cc\ub429\ub2c8\ub2e4.\n\n\uce74\ud2b8\ud3f4 \ud0dc\uc2a4\ud06c\ub294 \uc5d0\uc774\uc804\ud2b8\uc5d0 \ub300\ud55c \uc785\ub825\uc774 \ud658\uacbd \uc0c1\ud0dc(\uc704\uce58, \uc18d\ub3c4 \ub4f1)\ub97c \ub098\ud0c0\ub0b4\ub294 \n4\uac1c\uc758 \uc2e4\uc81c \uac12\uc774 \ub418\ub3c4\ub85d \uc124\uacc4\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc2e0\uacbd\ub9dd\uc740 \uc21c\uc218\ud558\uac8c \uadf8 \uc7a5\uba74\uc744 \ubcf4\uace0\n\ud0dc\uc2a4\ud06c\ub97c \ud574\uacb0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4 \ub530\ub77c\uc11c \uce74\ud2b8 \uc911\uc2ec\uc758 \ud654\uba74 \ud328\uce58\ub97c \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\uc774 \ub54c\ubb38\uc5d0 \uc6b0\ub9ac\uc758 \uacb0\uacfc\ub294 \uacf5\uc2dd \uc21c\uc704\ud45c\uc758 \uacb0\uacfc\uc640 \uc9c1\uc811\uc801\uc73c\ub85c \ube44\uad50\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4. \n\uc6b0\ub9ac\uc758 \ud0dc\uc2a4\ud06c\ub294 \ud6e8\uc52c \ub354 \uc5b4\ub835\uc2b5\ub2c8\ub2e4.\n\ubd88\ud589\ud788\ub3c4 \ubaa8\ub4e0 \ud504\ub808\uc784\uc744 \ub80c\ub354\ub9c1\ud574\uc57c\ub418\ubbc0\ub85c \uc774\uac83\uc740 \ud559\uc2b5 \uc18d\ub3c4\ub97c \ub2a6\ucd94\uac8c\ub429\ub2c8\ub2e4.\n\n\uc5c4\ubc00\ud788 \ub9d0\ud558\uba74, \ud604\uc7ac \uc2a4\ud06c\ub9b0 \ud328\uce58\uc640 \uc774\uc804 \uc2a4\ud06c\ub9b0 \ud328\uce58 \uc0ac\uc774\uc758 \ucc28\uc774\ub85c \uc0c1\ud0dc\ub97c \ud45c\uc2dc \ud560 \uac83\uc785\ub2c8\ub2e4.\n\uc774\ub807\uac8c\ud558\uba74 \uc5d0\uc774\uc804\ud2b8\uac00 \ub9c9\ub300\uc758 \uc18d\ub3c4\ub97c \ud55c \uc774\ubbf8\uc9c0\uc5d0\uc11c \uace0\ub824\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n**\ud328\ud0a4\uc9c0**\n\n\uba3c\uc800 \ud544\uc694\ud55c \ud328\ud0a4\uc9c0\ub97c \uac00\uc838\uc635\ub2c8\ub2e4. \uccab\uc9f8, \ud658\uacbd\uc744 \uc704\ud574 \n`gym <https://gym.openai.com/docs>`__ \uc774 \ud544\uc694\ud569\ub2c8\ub2e4.\n(`pip install gym` \uc744 \uc0ac\uc6a9\ud558\uc5ec \uc124\uce58\ud558\uc2ed\uc2dc\uc624).\n\ub610\ud55c PyTorch\uc5d0\uc11c \ub2e4\uc74c\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4:\n\n-  \uc2e0\uacbd\ub9dd (``torch.nn``)\n-  \ucd5c\uc801\ud654 (``torch.optim``)\n-  \uc790\ub3d9 \ubbf8\ubd84 (``torch.autograd``)\n-  \uc2dc\uac01 \ud0dc\uc2a4\ud06c\ub97c \uc704\ud55c \uc720\ud2f8\ub9ac\ud2f0\ub4e4 (``torchvision`` - `a separate\n   package <https://github.com/pytorch/vision>`__).\n\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "outputs": [],
      "source": [
        "import gym\nimport math\nimport random\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple\nfrom itertools import count\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as T\n\n\nenv = gym.make('CartPole-v0').unwrapped\n\n# matplotlib \uc124\uc815\nis_ipython = 'inline' in matplotlib.get_backend()\nif is_ipython:\n    from IPython import display\n\nplt.ion()\n\n# GPU\ub97c \uc0ac\uc6a9\ud560 \uacbd\uc6b0\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null
    },
    {
      "source": [
        "\uc7ac\ud604 \uba54\ubaa8\ub9ac(Replay Memory)\n-------------------------------\n\n\uc6b0\ub9ac\ub294 DQN \ud559\uc2b5\uc744 \uc704\ud574 \uacbd\ud5d8 \uc7ac\ud604 \uba54\ubaa8\ub9ac\ub97c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4.\n\uc5d0\uc774\uc804\ud2b8\uac00 \uad00\ucc30\ud55c \uc804\ud658(transition)\uc744 \uc800\uc7a5\ud574\uace0 \ub098\uc911\uc5d0 \uc774 \ub370\uc774\ud130\ub97c \n\uc7ac\uc0ac\uc6a9 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubb34\uc791\uc704\ub85c \uc0d8\ud50c\ub9c1\ud558\uba74 \ubc30\uce58\ub97c \uad6c\uc131\ud55c\ub294 \uc804\ud658\ub4e4\uc774\n\ube44\uc0c1\uad00(decorrelated)\ud558\uac8c \ub429\ub2c8\ub2e4. \uc774\uac83\uc774 DQN \ud559\uc2b5 \uc808\ucc28\ub97c \ud06c\uac8c \uc548\uc815\uc2dc\ud0a4\uace0\n\ud5a5\uc0c1\uc2dc\ud0a4\ub294 \uac83\uc73c\ub85c \ub098\ud0c0\ub0ac\uc2b5\ub2c8\ub2e4.\n\n\uc774\ub97c \uc704\ud574\uc11c \ub450\uac1c\uc758 \ud074\ub798\uc2a4\uac00 \ud544\uc694\ud569\ub2c8\ub2e4:\n\n-  ``Transition`` - \uc6b0\ub9ac \ud658\uacbd\uc5d0\uc11c \ub2e8\uc77c \uc804\ud658\uc744 \ub098\ud0c0\ub0b4\ub3c4\ub85d \uba85\uba85\ub41c \ud29c\ud50c\n-  ``ReplayMemory`` - \ucd5c\uadfc \uad00\ucc30\ub41c \uc804\uc774\ub97c \ubcf4\uad00 \uc720\uc9c0\ud558\ub294 \uc81c\ud55c\ub41c \ud06c\uae30\uc758 \uc21c\ud658 \ubc84\ud37c.\n   \ub610\ud55c \ud559\uc2b5\uc744 \uc704\ud55c \uc804\ud658\uc758 \ubb34\uc791\uc704 \ubc30\uce58\ub97c \uc120\ud0dd\ud558\uae30\uc704\ud55c\n   ``.sample ()`` \uba54\uc18c\ub4dc\ub97c \uad6c\ud604\ud569\ub2c8\ub2e4.\n\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition',\n                        ('state', 'action', 'next_state', 'reward'))\n\n\nclass ReplayMemory(object):\n\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n        self.position = 0\n\n    def push(self, *args):\n        \"\"\"\uc804\ud658 \uc800\uc7a5\"\"\"\n        if len(self.memory) < self.capacity:\n            self.memory.append(None)\n        self.memory[self.position] = Transition(*args)\n        self.position = (self.position + 1) % self.capacity\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null
    },
    {
      "source": [
        "\uc774\uc81c \ubaa8\ub378\uc744 \uc815\uc758\ud569\uc2dc\ub2e4. \uadf8\ub7ec\ub098 \uba3c\uc800 DQN\uc774 \ubb34\uc5c7\uc778\uc9c0 \uac04\ub2e8\ud788 \uc694\uc57d\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\nDQN \uc54c\uace0\ub9ac\uc998\n-------------\n\n\uc6b0\ub9ac\uc758 \ud658\uacbd\uc740 \uacb0\uc815\ub860\uc801\uc774\ubbc0\ub85c \uc5ec\uae30\uc5d0 \uc81c\uc2dc\ub41c \ubaa8\ub4e0 \ubc29\uc815\uc2dd\uc740 \ub2e8\uc21c\ud654\ub97c \uc704\ud574\n\uacb0\uc815\ub860\uc801\uc73c\ub85c \uacf5\uc2dd\ud654\ub429\ub2c8\ub2e4. \uac15\ud654 \ud559\uc2b5 \uc790\ub8cc\uc740 \ud658\uacbd\uc5d0\uc11c \ud655\ub960\ub860\uc801 \uc804\ud658\uc5d0 \n\ub300\ud55c \uae30\ub300\uac12(expectation)\ub3c4 \ud3ec\ud568 \ud560 \uac83\uc785\ub2c8\ub2e4.\n\n\uc6b0\ub9ac\uc758 \ubaa9\ud45c\ub294 \ud560\uc778\ub41c \ub204\uc801 \ubcf4\uc0c1 (discounted cumulative reward)\uc744 \n\uadf9\ub300\ud654\ud558\ub824\ub294 \uc815\ucc45(policy)\uc744 \ud559\uc2b5\ud558\ub294 \uac83\uc785\ub2c8\ub2e4.\n$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, \uc5ec\uae30\uc11c\n$R_{t_0}$ \ub294 *\ubc18\ud658(return)* \uc785\ub2c8\ub2e4. \ud560\uc778 \uc0c1\uc218,\n$\\gamma$, \ub294 $0$ \uacfc $1$ \uc758 \uc0c1\uc218\uc774\uace0 \ud569\uacc4\uac00 \n\uc218\ub834\ub418\ub3c4\ub85d \ubcf4\uc7a5\ud569\ub2c8\ub2e4. \uc5d0\uc774\uc804\ud2b8\uc5d0\uac8c \ubd88\ud655\uc2e4\ud55c \uba3c \ubbf8\ub798\uc758 \ubcf4\uc0c1\uc774\n\uac00\uae4c\uc6b4 \ubbf8\ub798\uc758 \uac83\uc5d0 \ube44\ud574 \ub35c \uc911\uc694\ud558\uac8c \ub9cc\ub4e4\uace0, \uc774\uac83\uc740 \uc0c1\ub2f9\ud788 \ud569\ub9ac\uc801\uc785\ub2c8\ub2e4.\n\nQ-learning\uc758 \uc8fc\uc694 \uc544\uc774\ub514\uc5b4\ub294 \ub9cc\uc77c \ud568\uc218 $Q^*: State \\times Action \\rightarrow \\mathbb{R}$ \ub97c\n\uac00\uc9c0\uace0 \uc788\ub2e4\uba74 \ubc18\ud658\uc774 \uc5b4\ub36f\uac8c \ub420\uc9c0 \uc54c\ub824\uc904 \uc218 \uc788\uace0, \n\ub9cc\uc57d \uc8fc\uc5b4\uc9c4 \uc0c1\ud0dc(state)\uc5d0\uc11c \ud589\ub3d9(action)\uc744 \ud55c\ub2e4\uba74, \ubcf4\uc0c1\uc744 \ucd5c\ub300\ud654\ud558\ub294 \n\uc815\ucc45\uc744 \uc27d\uac8c \uad6c\ucd95\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n\n\uadf8\ub7ec\ub098 \uc138\uacc4(world)\uc5d0 \uad00\ud55c \ubaa8\ub4e0 \uac83\uc744 \uc54c\uc9c0 \ubabb\ud558\uae30 \ub54c\ubb38\uc5d0, \n$Q^*$ \uc5d0 \ub3c4\ub2ec\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc2e0\uacbd\ub9dd\uc740 \n\ubc94\uc6a9 \ud568\uc218 \uadfc\uc0ac\uc790(universal function approximator)\uc774\uae30 \ub54c\ubb38\uc5d0\n\uac04\ub2e8\ud558\uac8c \uc0dd\uc131\ud558\uace0 $Q^*$ \ub97c \ub2ee\ub3c4\ub85d \ud559\uc2b5 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n\n\ud559\uc2b5 \uc5c5\ub370\uc774\ud2b8 \uaddc\uce59\uc73c\ub85c, \uc77c\ubd80 \uc815\ucc45\uc744 \uc704\ud55c \ubaa8\ub4e0 $Q$ \ud568\uc218\uac00 \nBellman \ubc29\uc815\uc2dd\uc744 \uc900\uc218\ud55c\ub2e4\ub294 \uc0ac\uc2e4\uc744 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4:\n\n\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n\n\ud3c9\ub4f1(equality)\uc758 \ub450 \uce21\uba74 \uc0ac\uc774\uc758 \ucc28\uc774\ub294 \n\uc2dc\uac04\ucc28 \uc624\ub958(temporal difference error), $\\delta$ \uc785\ub2c8\ub2e4.:\n\n\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}\n\n\uc624\ub958\ub97c \ucd5c\uc18c\ud654\ud558\uae30 \uc704\ud574\uc11c `Huber\nloss <https://en.wikipedia.org/wiki/Huber_loss>`__ \ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\nHuber loss \ub294 \uc624\ub958\uac00 \uc791\uc73c\uba74 \ud3c9\uade0 \uc81c\uacf1 \uc624\ucc28( mean squared error)\uc640 \uac19\uc774\n\ub3d9\uc791\ud558\uace0 \uc624\ub958\uac00 \ud074 \ub54c\ub294 \ud3c9\uade0 \uc808\ub300 \uc624\ub958\uc640 \uc720\uc0ac\ud569\ub2c8\ub2e4.\n- \uc774\uac83\uc740 $Q$ \uc758 \ucd94\uc815\uc774 \ub9e4\uc6b0 \ud63c\ub780\uc2a4\ub7ec\uc6b8 \ub54c \uc774\uc0c1 \uac12\uc5d0 \ub354 \uac15\uac74\ud558\uac8c \ud569\ub2c8\ub2e4.\n\uc7ac\ud604 \uba54\ubaa8\ub9ac\uc5d0\uc11c \uc0d8\ud50c\ub9c1\ud55c \uc804\ud658 \ubc30\uce58 $B$ \uc5d0\uc11c \uc774\uac83\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4:\n\n\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n\n\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n   \\end{cases}\\end{align}\n\nQ-\ub124\ud2b8\uc6cc\ud06c\n^^^^^^^^^^^\n\n\uc6b0\ub9ac \ubaa8\ub378\uc740 \ud604\uc7ac\uc640 \uc774\uc804 \uc2a4\ud06c\ub9b0 \ud328\uce58\uc758 \ucc28\uc774\ub97c \ucde8\ud558\ub294 \nCNN(convolutional neural network) \uc785\ub2c8\ub2e4. \ub450\uac00\uc9c0 \ucd9c\ub825 $Q(s, \\mathrm{left})$ \uc640\n$Q(s, \\mathrm{right})$ \uac00 \uc788\uc2b5\ub2c8\ub2e4. (\uc5ec\uae30\uc11c $s$ \ub294 \ub124\ud2b8\uc6cc\ud06c\uc758 \uc785\ub825\uc785\ub2c8\ub2e4)\n\uacb0\uacfc\uc801\uc73c\ub85c \ub124\ud2b8\uc6cc\ud06c\ub294 \uc8fc\uc5b4\uc9c4 \ud604\uc7ac \uc785\ub825\uc5d0\uc11c \uac01 \ud589\ub3d9\uc758 *\ud488\uc9c8* \uc744 \uc608\uce21\ud558\ub824\uace0 \ud569\ub2c8\ub2e4.\n\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n\n    def __init__(self):\n        super(DQN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n        self.bn2 = nn.BatchNorm2d(32)\n        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n        self.bn3 = nn.BatchNorm2d(32)\n        self.head = nn.Linear(448, 2)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        return self.head(x.view(x.size(0), -1))"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null
    },
    {
      "source": [
        "\uc785\ub825 \ucd94\ucd9c\n^^^^^^^^^^^^^^^^\n\n\uc544\ub798 \ucf54\ub4dc\ub294 \ud658\uacbd\uc5d0\uc11c \ub80c\ub354\ub9c1 \ub41c \uc774\ubbf8\uc9c0\ub97c \ucd94\ucd9c\ud558\uace0 \ucc98\ub9ac\ud558\ub294 \uc720\ud2f8\ub9ac\ud2f0\uc785\ub2c8\ub2e4.\n\uc774\ubbf8\uc9c0 \ubcc0\ud658\uc744 \uc27d\uac8c \uad6c\uc131 \ud560 \uc218 \uc788\ub294 ``torchvision`` \ud328\ud0a4\uc9c0\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \n\uc140(cell)\uc744 \uc2e4\ud589\ud558\uba74 \ucd94\ucd9c\ud55c \uc608\uc81c \ud328\uce58\uac00 \ud45c\uc2dc\ub429\ub2c8\ub2e4.\n\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "outputs": [],
      "source": [
        "resize = T.Compose([T.ToPILImage(),\n                    T.Resize(40, interpolation=Image.CUBIC),\n                    T.ToTensor()])\n\n# \uc774\uac83\uc740 gym \ucf54\ub4dc\ub97c \uae30\ubc18\uc73c\ub85c \ud569\ub2c8\ub2e4.\nscreen_width = 600\n\n\ndef get_cart_location():\n    world_width = env.x_threshold * 2\n    scale = screen_width / world_width\n    return int(env.state[0] * scale + screen_width / 2.0)  # \uce74\ud2b8\uc758 \uc911\uac04\n\n\ndef get_screen():\n    screen = env.render(mode='rgb_array').transpose(\n        (2, 0, 1))  # transpose into torch order (CHW)\n    # Strip off the top and bottom of the screen\n    screen = screen[:, 160:320]\n    view_width = 320\n    cart_location = get_cart_location()\n    if cart_location < view_width // 2:\n        slice_range = slice(view_width)\n    elif cart_location > (screen_width - view_width // 2):\n        slice_range = slice(-view_width, None)\n    else:\n        slice_range = slice(cart_location - view_width // 2,\n                            cart_location + view_width // 2)\n    # Strip off the edges, so that we have a square image centered on a cart\n    screen = screen[:, :, slice_range]\n    # Convert to float, rescare, convert to torch tensor\n    # (this doesn't require a copy)\n    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n    screen = torch.from_numpy(screen)\n    # Resize, and add a batch dimension (BCHW)\n    return resize(screen).unsqueeze(0).to(device)\n\n\nenv.reset()\nplt.figure()\nplt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n           interpolation='none')\nplt.title('Example extracted screen')\nplt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null
    },
    {
      "source": [
        "\ud559\uc2b5\n--------\n\n\ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\uc640 \uc720\ud2f8\ub9ac\ud2f0\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\uc774 \uc140\uc740 \ubaa8\ub378\uacfc \ucd5c\uc801\ud654\uae30\ub97c \uc778\uc2a4\ud134\uc2a4\ud654\ud558\uace0 \uc77c\ubd80 \uc720\ud2f8\ub9ac\ud2f0\ub97c \uc815\uc758\ud569\ub2c8\ub2e4:\n\n-  ``select_action`` - Epsilon Greedy \uc815\ucc45\uc5d0 \ub530\ub77c \ud589\ub3d9\uc744 \uc120\ud0dd\ud569\ub2c8\ub2e4.\n   \uac04\ub2e8\ud788 \ub9d0\ud574\uc11c, \uac00\ub054 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uc5ec \ud589\ub3d9\uc744 \uc120\ud0dd\ud558\uace0 \ub54c\ub85c\ub294 \ub2e8\uc9c0 \ud558\ub098\ub97c\n   \uade0\uc77c\ud558\uac8c \uc0d8\ud50c\ub9c1 \ud560 \uac83\uc785\ub2c8\ub2e4. \uc784\uc758\uc758 \uc561\uc158\uc744 \uc120\ud0dd\ud560 \ud655\ub960\uc740 \n   ``EPS_START`` \uc5d0\uc11c \uc2dc\uc791\ud574\uc11c ``EPS_END`` \ub97c \ud5a5\ud574 \uc9c0\uc218\uc801\uc73c\ub85c \uac10\uc18c \ud560 \uac83\uc785\ub2c8\ub2e4.\n   ``EPS_DECAY``\ub294 \uac10\uc1e0 \uc18d\ub3c4\ub97c \uc81c\uc5b4\ud569\ub2c8\ub2e4.\n-  ``plot_durations`` - \uc9c0\ub09c 100\uac1c \uc5d0\ud53c\uc18c\ub4dc\uc758 \ud3c9\uade0(\uacf5\uc2dd \ud3c9\uac00\uc5d0\uc11c \uc0ac\uc6a9 \ub41c \uc218\uce58)\uc5d0 \ub530\ub978\n   \uc5d0\ud53c\uc18c\ub4dc\uc758 \uc9c0\uc18d\uc744 \ub3c4\ud45c\ub85c \uadf8\ub9ac\uae30 \uc704\ud55c \ud5ec\ud37c. \ub3c4\ud45c\ub294 \uae30\ubcf8 \ud6c8\ub828 \ub8e8\ud504\uac00 \n   \ud3ec\ud568 \ub41c \uc140 \ubc11\uc5d0 \uc788\uc73c\uba70, \ub9e4 \uc5d0\ud53c\uc18c\ub4dc\ub9c8\ub2e4 \uc5c5\ub370\uc774\ud2b8\ub429\ub2c8\ub2e4.\n\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\nGAMMA = 0.999\nEPS_START = 0.9\nEPS_END = 0.05\nEPS_DECAY = 200\nTARGET_UPDATE = 10\n\npolicy_net = DQN().to(device)\ntarget_net = DQN().to(device)\ntarget_net.load_state_dict(policy_net.state_dict())\ntarget_net.eval()\n\noptimizer = optim.RMSprop(policy_net.parameters())\nmemory = ReplayMemory(10000)\n\n\nsteps_done = 0\n\n\ndef select_action(state):\n    global steps_done\n    sample = random.random()\n    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n        math.exp(-1. * steps_done / EPS_DECAY)\n    steps_done += 1\n    if sample > eps_threshold:\n        with torch.no_grad():\n            return policy_net(state).max(1)[1].view(1, 1)\n    else:\n        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)\n\n\nepisode_durations = []\n\n\ndef plot_durations():\n    plt.figure(2)\n    plt.clf()\n    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n    plt.title('Training...')\n    plt.xlabel('Episode')\n    plt.ylabel('Duration')\n    plt.plot(durations_t.numpy())\n    # 100\uac1c\uc758 \uc5d0\ud53c\uc18c\ub4dc \ud3c9\uade0\uc744 \uac00\uc838 \uc640\uc11c \ub3c4\ud45c \uadf8\ub9ac\uae30\n    if len(durations_t) >= 100:\n        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n        means = torch.cat((torch.zeros(99), means))\n        plt.plot(means.numpy())\n\n    plt.pause(0.001)  # \ub3c4\ud45c\uac00 \uc5c5\ub370\uc774\ud2b8\ub418\ub3c4\ub85d \uc7a0\uc2dc \uba48\ucda4 \n    if is_ipython:\n        display.clear_output(wait=True)\n        display.display(plt.gcf())"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null
    },
    {
      "source": [
        "\ud559\uc2b5 \ub8e8\ud504\n^^^^^^^^^^^^^\n\n\ucd5c\uc885\uc801\uc73c\ub85c \ubaa8\ub378 \ud559\uc2b5\uc744 \uc704\ud55c \ucf54\ub4dc.\n\n\uc5ec\uae30\uc11c, \ucd5c\uc801\ud654\uc758 \ud55c \ub2e8\uacc4\ub97c \uc218\ud589\ud558\ub294 ``optimize_model`` \ud568\uc218\ub97c \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uba3c\uc800 \ubc30\uce58 \ud558\ub098\ub97c \uc0d8\ud50c\ub9c1\ud558\uace0 \ubaa8\ub4e0 Tensor\ub97c \ud558\ub098\ub85c \uc5f0\uacb0\ud558\uace0 \n$Q(s_t, a_t)$ \uc640  $V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$ \ub97c \uacc4\uc0b0\ud558\uace0\n\uadf8\uac83\ub4e4\uc744 \uc190\uc2e4\ub85c \ud569\uce69\ub2c8\ub2e4. \uc6b0\ub9ac\uac00 \uc124\uc815\ud55c \uc815\uc758\ub97c \ub530\ub974\uba74 \ub9cc\uc57d $s$ \uac00\n\ub9c8\uc9c0\ub9c9 \uc0c1\ud0dc\ub77c\uba74 $V(s) = 0$ \uc774\ub2e4.\n\ub610\ud55c \uc548\uc815\uc131 \ucd94\uac00 \uc704\ud55c $V(s_{t+1})$ \uacc4\uc0b0\uc744 \uc704\ud574 \ubaa9\ud45c \ub124\ud2b8\uc6cc\ud06c\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \n\ubaa9\ud45c \ub124\ud2b8\uc6cc\ud06c\ub294 \ub300\ubd80\ubd84\uc758 \uc2dc\uac04 \ub3d9\uacb0 \uc0c1\ud0dc\ub85c \uc720\uc9c0\ub418\uc9c0\ub9cc, \uac00\ub054 \uc815\ucc45 \n\ub124\ud2b8\uc6cc\ud06c\uc758 \uac00\uc911\uce58\ub85c \uc5c5\ub370\uc774\ud2b8\ub429\ub2c8\ub2e4.\n\uc774\uac83\uc740 \ub300\uac1c \uc124\uc815\ud55c \uc2a4\ud15d \uc22b\uc790\uc774\uc9c0\ub9cc \ub2e8\uc21c\ud654\ub97c \uc704\ud574 \uc5d0\ud53c\uc18c\ub4dc\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "outputs": [],
      "source": [
        "def optimize_model():\n    if len(memory) < BATCH_SIZE:\n        return\n    transitions = memory.sample(BATCH_SIZE)\n    # Transpose the batch (\uc790\uc138\ud55c \uc124\uba85\uc744 \uc704\ud574 http://stackoverflow.com/a/19343/3343043 \ub97c \ubcf4\uc2ed\uc2dc\uc624).\n    batch = Transition(*zip(*transitions))\n\n    # \ucd5c\uc885 \uc0c1\ud0dc\uac00 \uc544\ub2cc \ub9c8\uc2a4\ud06c\ub97c \uacc4\uc0b0\ud558\uace0 \ubc30\uce58 \uc694\uc18c\ub97c \uc5f0\uacb0\ud569\ub2c8\ub2e4.\n    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n                                          batch.next_state)), device=device, dtype=torch.uint8)\n    non_final_next_states = torch.cat([s for s in batch.next_state\n                                                if s is not None])\n    state_batch = torch.cat(batch.state)\n    action_batch = torch.cat(batch.action)\n    reward_batch = torch.cat(batch.reward)\n\n    # Q(s_t, a) \uacc4\uc0b0 - \ubaa8\ub378\uc774 Q(s_t)\ub97c \uacc4\uc0b0\ud558\uace0, \ucde8\ud55c \ud589\ub3d9\uc758 \uce7c\ub7fc\uc744 \uc120\ud0dd\ud55c\ub2e4.\n    state_action_values = policy_net(state_batch).gather(1, action_batch)\n\n    # \ubaa8\ub4e0 \ub2e4\uc74c \uc0c1\ud0dc\ub97c \uc704\ud55c V(s_{t+1}) \uacc4\uc0b0\n    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n    # \uae30\ub300 Q \uac12 \uacc4\uc0b0\n    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n\n    # Huber \uc190\uc2e4 \uacc4\uc0b0\n    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n\n    # \ubaa8\ub378 \ucd5c\uc801\ud654\n    optimizer.zero_grad()\n    loss.backward()\n    for param in policy_net.parameters():\n        param.grad.data.clamp_(-1, 1)\n    optimizer.step()"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null
    },
    {
      "source": [
        "\uc544\ub798\uc5d0\uc11c \uc8fc\uc694 \ud559\uc2b5 \ub8e8\ud504\ub97c \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ucc98\uc74c\uc73c\ub85c \ud658\uacbd\uc744 \n\uc7ac\uc124\uc815\ud558\uace0 ``\uc0c1\ud0dc`` Tensor\ub97c \ucd08\uae30\ud654\ud569\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c \ud589\ub3d9\uc744\n\uc0d8\ud50c\ub9c1\ud558\uace0, \uadf8\uac83\uc744 \uc2e4\ud589\ud558\uace0, \ub2e4\uc74c \ud654\uba74\uacfc \ubcf4\uc0c1(\ud56d\uc0c1 1)\uc744 \uad00\ucc30\ud558\uace0,\n\ubaa8\ub378\uc744 \ud55c \ubc88 \ucd5c\uc801\ud654\ud569\ub2c8\ub2e4. \uc5d0\ud53c\uc18c\ub4dc\uac00 \ub05d\ub098\uba74 (\ubaa8\ub378\uc774 \uc2e4\ud328) \n\ub8e8\ud504\ub97c \ub2e4\uc2dc \uc2dc\uc791\ud569\ub2c8\ub2e4.\n\n\uc544\ub798\uc5d0\uc11c `num_episodes` \ub294 \uc791\uac8c \uc124\uc815\ub429\ub2c8\ub2e4. \ub178\ud2b8\ubd81\uc744 \ub2e4\uc6b4\ubc1b\uace0\n\ub354\ub9ce\uc740 \uc5d0\ud53c\uc18c\ub4dc\ub97c \uc2e4\ud589\ud574 \ubcf4\uc2ed\uc2dc\uc624\n\n\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "outputs": [],
      "source": [
        "num_episodes = 50\nfor i_episode in range(num_episodes):\n    # Initialize the environment and state\n    env.reset()\n    last_screen = get_screen()\n    current_screen = get_screen()\n    state = current_screen - last_screen\n    for t in count():\n        # \ud589\ub3d9 \uc120\ud0dd\uacfc \uc218\ud589\n        action = select_action(state)\n        _, reward, done, _ = env.step(action.item())\n        reward = torch.tensor([reward], device=device)\n\n        # \uc0c8\ub85c\uc6b4 \uc0c1\ud0dc \uad00\ucc30\n        last_screen = current_screen\n        current_screen = get_screen()\n        if not done:\n            next_state = current_screen - last_screen\n        else:\n            next_state = None\n\n        # \uba54\ubaa8\ub9ac\uc5d0 \ubcc0\uc774 \uc800\uc7a5\n        memory.push(state, action, next_state, reward)\n\n        # \ub2e4\uc74c \uc0c1\ud0dc\ub85c \uc774\ub3d9\n        state = next_state\n\n        # \ucd5c\uc801\ud654 \ud55c\ub2e8\uacc4 \uc218\ud589(\ubaa9\ud45c \ub124\ud2b8\uc6cc\ud06c\uc5d0\uc11c)\n        optimize_model()\n        if done:\n            episode_durations.append(t + 1)\n            plot_durations()\n            break\n    # \ubaa9\ud45c \ub124\ud2b8\uc6cc\ud06c \uc5c5\ub370\uc774\ud2b8\n    if i_episode % TARGET_UPDATE == 0:\n        target_net.load_state_dict(policy_net.state_dict())\n\nprint('Complete')\nenv.render()\nenv.close()\nplt.ioff()\nplt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "collapsed": false
      },
      "execution_count": null
    }
  ],
  "nbformat": 4,
  "metadata": {
    "language_info": {
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "mimetype": "text/x-python",
      "version": "3.5.0",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat_minor": 0
}